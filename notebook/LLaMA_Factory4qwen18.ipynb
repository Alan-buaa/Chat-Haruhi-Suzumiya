{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Chat-Haruhi-Suzumiya/blob/main/notebook/LLaMA_Factory4qwen18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rfBB8kjW1d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a10adfb-1160-4a0e-c409-4cd34bec130c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade huggingface_hub\n",
        "! huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/hhhwmws0117/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amEnYxx7nXfY",
        "outputId": "682b0a80-7ba5-4c1e-b8cd-d122dbbfc5a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 5112, done.\u001b[K\n",
            "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 5112 (delta 234), reused 271 (delta 200), pack-reused 4766\u001b[K\n",
            "Receiving objects: 100% (5112/5112), 184.66 MiB | 17.05 MiB/s, done.\n",
            "Resolving deltas: 100% (3628/3628), done.\n",
            "Updating files: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -r requirements.txt\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "7H-obXT42Pmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae28c69-90aa-421e-b53a-f563851aeb8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu118)\n",
            "Collecting transformers<4.35.0,>=4.31.0 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.14.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.6.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.7.4 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.20.3)\n",
            "Collecting tiktoken (from -r requirements.txt (line 11))\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.42.1)\n",
            "Collecting rouge-chinese (from -r requirements.txt (line 13))\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.8.1)\n",
            "Collecting uvicorn (from -r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.10.13)\n",
            "Collecting fastapi (from -r requirements.txt (line 17))\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 18))\n",
            "  Downloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.7.4->-r requirements.txt (line 6))\n",
            "  Downloading tyro-0.6.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.2.2)\n",
            "Collecting ffmpy (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (9.4.0)\n",
            "Collecting pydub (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->-r requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 14)) (1.3.2)\n",
            "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 17)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->-r requirements.txt (line 17))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (2.8.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 17)) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 17)) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2)) (2023.11.17)\n",
            "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.50.1-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.50.0-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.49.0-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.47.1-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.6.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.6.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.47.0-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading gradio-3.46.1-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.5.3 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.5.3-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.46.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.45.2-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.45.1-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.5.2 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.5.2-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.45.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.44.4-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.5.1 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.5.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.44.3-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.5.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.44.2-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.44.1-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.44.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.43.2-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.43.1-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.43.0-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.42.0-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.41.2-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.41.1-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.41.0-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client>=0.4.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (3.0.0)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client>=0.4.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.40.0-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client>=0.3.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client>=0.2.10 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.6.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.6.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.14.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4 (from transformers<4.35.0,>=4.31.0->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.7.4->-r requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.7.4->-r requirements.txt (line 6)) (13.7.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.7.4->-r requirements.txt (line 6))\n",
            "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.4->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=1d7d6f961b1b4e24281d17320b3e53622f301a07818fdac7c056bd61cfc2161e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, ffmpy, websockets, typing-extensions, shtab, semantic-version, rouge-chinese, python-multipart, pyarrow-hotfix, orjson, h11, docstring-parser, dill, aiofiles, uvicorn, tiktoken, starlette, multiprocess, huggingface-hub, httpcore, tyro, tokenizers, httpx, fastapi, accelerate, transformers, sse-starlette, gradio-client, datasets, trl, peft, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 aiofiles-23.2.1 datasets-2.14.7 dill-0.3.7 docstring-parser-0.15 fastapi-0.104.1 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.17.3 multiprocess-0.70.15 orjson-3.9.10 peft-0.6.2 pyarrow-hotfix-0.6 pydub-0.25.1 python-multipart-0.0.6 rouge-chinese-1.0.3 semantic-version-2.10.0 sentencepiece-0.1.99 shtab-1.6.5 sse-starlette-1.8.2 starlette-0.27.0 tiktoken-0.5.2 tokenizers-0.14.1 transformers-4.34.1 trl-0.7.4 typing-extensions-4.8.0 tyro-0.6.0 uvicorn-0.24.0.post1 websockets-11.0.3\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m718.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z1twnn2EbRh",
        "outputId": "11080b66-3872-4911-8382-9bf75929ae13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers_stream_generator\n",
            "  Downloading transformers-stream-generator-0.0.4.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.26.1->transformers_stream_generator) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.26.1->transformers_stream_generator) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2023.11.17)\n",
            "Building wheels for collected packages: transformers_stream_generator\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.4-py3-none-any.whl size=12316 sha256=6eb797f3369d285846c77487c4daee45c72fe771ad0d985cda67c2d6ee17925c\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/1d/3c/92d88493ed40c0d9be60a391eb76c9a56e9f9b7542cb789401\n",
            "Successfully built transformers_stream_generator\n",
            "Installing collected packages: transformers_stream_generator\n",
            "Successfully installed transformers_stream_generator-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path  Qwen/Qwen-1_8B-Chat\\\n",
        "    --do_train True\\\n",
        "    --dataset chatharuhi \\\n",
        "    --template qwen \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target c_attn \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --output_dir phi-1_5-finetuned \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 100 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16 True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhGdi7BW2q_T",
        "outputId": "1f92f404-00e3-4914-b61a-e36df9328f00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-04 09:02:08.059638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-04 09:02:08.059712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-04 09:02:08.059765: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-04 09:02:09.312764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "12/04/2023 09:02:11 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-12-04 09:02:11,450 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-12-04 09:02:11,450 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "12/04/2023 09:02:11 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "12/04/2023 09:02:11 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=phi-1_5-finetuned/runs/Dec04_09-02-11_b8e4a37fda96,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=100,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=phi-1_5-finetuned,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=phi-1_5-finetuned,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "12/04/2023 09:02:11 - INFO - llmtuner.data.loader - Loading dataset silk-road/ChatHaruhi-English-62K-RolePlaying...\n",
            "Using custom data configuration default-aee6a3dcb2798cbe\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/silk-road___chat_haruhi-english-62_k-role_playing/default-aee6a3dcb2798cbe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset chat_haruhi-english-62_k-role_playing (/root/.cache/huggingface/datasets/silk-road___chat_haruhi-english-62_k-role_playing/default-aee6a3dcb2798cbe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/silk-road___chat_haruhi-english-62_k-role_playing/default-aee6a3dcb2798cbe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-12-04 09:02:12,851 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-12-04 09:02:12,851 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-12-04 09:02:12,852 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-12-04 09:02:12,852 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-12-04 09:02:12,852 >> loading file tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2023-12-04 09:02:13,565 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/config.json\n",
            "[INFO|configuration_utils.py:715] 2023-12-04 09:02:13,675 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-12-04 09:02:13,676 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "Downloading (…)_generation_utils.py: 100% 14.6k/14.6k [00:00<00:00, 66.7MB/s]\n",
            "Downloading cpp_kernels.py: 100% 1.92k/1.92k [00:00<00:00, 7.93MB/s]\n",
            "Downloading (…)fetensors.index.json: 100% 14.7k/14.7k [00:00<00:00, 57.0MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2023-12-04 09:02:14,241 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading (…)of-00002.safetensors:   0% 0.00/2.04G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   1% 10.5M/2.04G [00:00<00:21, 96.3MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   2% 41.9M/2.04G [00:00<00:10, 187MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:   4% 73.4M/2.04G [00:00<00:09, 214MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   5% 105M/2.04G [00:00<00:08, 231MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:   7% 136M/2.04G [00:00<00:08, 235MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   8% 168M/2.04G [00:00<00:07, 238MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  10% 199M/2.04G [00:00<00:07, 231MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  11% 231M/2.04G [00:01<00:08, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  13% 262M/2.04G [00:01<00:08, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  14% 294M/2.04G [00:01<00:07, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  16% 325M/2.04G [00:01<00:07, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  17% 357M/2.04G [00:01<00:07, 223MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  19% 388M/2.04G [00:01<00:07, 228MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  21% 419M/2.04G [00:01<00:07, 223MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  22% 451M/2.04G [00:02<00:07, 219MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  24% 482M/2.04G [00:02<00:07, 217MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 514M/2.04G [00:02<00:06, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  27% 545M/2.04G [00:02<00:06, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 577M/2.04G [00:02<00:07, 204MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  29% 598M/2.04G [00:02<00:07, 200MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  30% 619M/2.04G [00:02<00:07, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  31% 640M/2.04G [00:02<00:07, 191MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  32% 661M/2.04G [00:03<00:07, 189MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  33% 682M/2.04G [00:03<00:09, 136MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  34% 703M/2.04G [00:03<00:09, 147MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  35% 724M/2.04G [00:03<00:08, 157MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  37% 755M/2.04G [00:03<00:07, 177MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  39% 786M/2.04G [00:03<00:06, 192MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 818M/2.04G [00:04<00:06, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  41% 839M/2.04G [00:04<00:06, 187MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  42% 860M/2.04G [00:04<00:06, 187MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  43% 881M/2.04G [00:04<00:06, 186MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  44% 902M/2.04G [00:04<00:06, 175MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  45% 923M/2.04G [00:04<00:06, 173MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  47% 954M/2.04G [00:04<00:05, 187MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  48% 986M/2.04G [00:04<00:05, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  50% 1.02G/2.04G [00:05<00:05, 203MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  51% 1.05G/2.04G [00:05<00:04, 213MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  53% 1.08G/2.04G [00:05<00:04, 213MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  55% 1.11G/2.04G [00:05<00:04, 206MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 1.13G/2.04G [00:05<00:04, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 1.15G/2.04G [00:05<00:04, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  58% 1.18G/2.04G [00:05<00:04, 202MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  59% 1.21G/2.04G [00:06<00:04, 202MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  61% 1.24G/2.04G [00:06<00:03, 211MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  62% 1.27G/2.04G [00:06<00:03, 219MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  64% 1.30G/2.04G [00:06<00:03, 226MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  65% 1.33G/2.04G [00:06<00:03, 227MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  67% 1.36G/2.04G [00:06<00:02, 232MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  68% 1.39G/2.04G [00:06<00:02, 233MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  70% 1.43G/2.04G [00:06<00:02, 236MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  71% 1.46G/2.04G [00:07<00:02, 227MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 1.49G/2.04G [00:07<00:02, 223MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  75% 1.52G/2.04G [00:07<00:02, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  76% 1.55G/2.04G [00:07<00:02, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  78% 1.58G/2.04G [00:08<00:05, 85.3MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  79% 1.61G/2.04G [00:08<00:04, 105MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:  80% 1.64G/2.04G [00:08<00:03, 111MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  81% 1.66G/2.04G [00:08<00:03, 110MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  82% 1.68G/2.04G [00:09<00:02, 124MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  84% 1.71G/2.04G [00:09<00:02, 142MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  85% 1.74G/2.04G [00:09<00:01, 158MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  87% 1.77G/2.04G [00:09<00:01, 175MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  88% 1.80G/2.04G [00:09<00:01, 191MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  90% 1.84G/2.04G [00:09<00:01, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  92% 1.87G/2.04G [00:09<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  93% 1.90G/2.04G [00:10<00:00, 211MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  95% 1.93G/2.04G [00:10<00:00, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 1.96G/2.04G [00:10<00:00, 227MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  98% 1.99G/2.04G [00:10<00:00, 235MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors: 100% 2.04G/2.04G [00:10<00:00, 191MB/s]\n",
            "Downloading shards:  50% 1/2 [00:10<00:10, 10.90s/it]\n",
            "Downloading (…)of-00002.safetensors:   0% 0.00/1.63G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   2% 31.5M/1.63G [00:00<00:06, 237MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   4% 62.9M/1.63G [00:00<00:07, 217MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   6% 94.4M/1.63G [00:00<00:07, 215MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   8% 126M/1.63G [00:00<00:06, 219MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:  10% 157M/1.63G [00:00<00:06, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  12% 189M/1.63G [00:00<00:06, 219MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  13% 220M/1.63G [00:01<00:06, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  15% 252M/1.63G [00:01<00:06, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  17% 283M/1.63G [00:01<00:05, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  19% 315M/1.63G [00:01<00:05, 229MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  21% 346M/1.63G [00:01<00:05, 230MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  23% 377M/1.63G [00:01<00:05, 236MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 409M/1.63G [00:01<00:05, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  27% 440M/1.63G [00:01<00:05, 214MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  29% 472M/1.63G [00:02<00:05, 214MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  31% 503M/1.63G [00:02<00:05, 211MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  33% 535M/1.63G [00:02<00:05, 220MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  35% 566M/1.63G [00:02<00:05, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  36% 587M/1.63G [00:02<00:05, 175MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  38% 619M/1.63G [00:02<00:05, 188MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 650M/1.63G [00:03<00:04, 199MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  42% 682M/1.63G [00:03<00:04, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  43% 703M/1.63G [00:03<00:04, 200MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  44% 724M/1.63G [00:03<00:04, 196MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  46% 755M/1.63G [00:03<00:04, 203MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  48% 786M/1.63G [00:03<00:04, 207MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  50% 818M/1.63G [00:03<00:03, 211MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  52% 849M/1.63G [00:04<00:03, 214MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  54% 881M/1.63G [00:04<00:03, 216MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 912M/1.63G [00:04<00:03, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  58% 944M/1.63G [00:04<00:03, 223MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  60% 975M/1.63G [00:04<00:02, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  62% 1.01G/1.63G [00:04<00:02, 228MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  64% 1.04G/1.63G [00:04<00:02, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  65% 1.07G/1.63G [00:04<00:02, 223MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  67% 1.10G/1.63G [00:05<00:02, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  69% 1.13G/1.63G [00:05<00:02, 205MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  71% 1.15G/1.63G [00:05<00:02, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  72% 1.17G/1.63G [00:05<00:02, 178MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 1.20G/1.63G [00:05<00:02, 177MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  74% 1.22G/1.63G [00:05<00:02, 173MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  76% 1.24G/1.63G [00:05<00:02, 180MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  77% 1.26G/1.63G [00:06<00:02, 175MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  78% 1.28G/1.63G [00:07<00:08, 40.7MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  80% 1.31G/1.63G [00:07<00:05, 59.1MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  82% 1.34G/1.63G [00:07<00:03, 78.3MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  83% 1.36G/1.63G [00:08<00:03, 84.3MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  85% 1.38G/1.63G [00:08<00:02, 98.7MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  86% 1.41G/1.63G [00:08<00:02, 88.9MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  87% 1.43G/1.63G [00:08<00:01, 106MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:  89% 1.45G/1.63G [00:08<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  90% 1.48G/1.63G [00:08<00:01, 147MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  92% 1.51G/1.63G [00:08<00:00, 165MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  94% 1.54G/1.63G [00:09<00:00, 179MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 1.56G/1.63G [00:09<00:00, 185MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  98% 1.59G/1.63G [00:09<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors: 100% 1.63G/1.63G [00:09<00:00, 171MB/s]\n",
            "Downloading shards: 100% 2/2 [00:20<00:00, 10.28s/it]\n",
            "[INFO|modeling_utils.py:1220] 2023-12-04 09:02:34,797 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:770] 2023-12-04 09:02:34,799 >> Generate config GenerationConfig {}\n",
            "\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.71s/it]\n",
            "[INFO|modeling_utils.py:3775] 2023-12-04 09:02:38,716 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3783] 2023-12-04 09:02:38,716 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "Downloading generation_config.json: 100% 249/249 [00:00<00:00, 1.27MB/s]\n",
            "[INFO|configuration_utils.py:730] 2023-12-04 09:02:38,834 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/4e808f3e25c532cce2a7df9f9d705996ecc0789d/generation_config.json\n",
            "[INFO|configuration_utils.py:770] 2023-12-04 09:02:38,835 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "12/04/2023 09:02:38 - INFO - llmtuner.model.utils - Gradient checkpointing enabled.\n",
            "12/04/2023 09:02:38 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "12/04/2023 09:02:38 - INFO - llmtuner.model.loader - trainable params: 3145728 || all params: 1839974400 || trainable%: 0.1710\n",
            "12/04/2023 09:02:38 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "12/04/2023 09:02:38 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "Running tokenizer on dataset:   0% 0/62362 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/silk-road___chat_haruhi-english-62_k-role_playing/default-aee6a3dcb2798cbe/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bbd9c7c7df890a75.arrow\n",
            "Running tokenizer on dataset: 100% 62362/62362 [02:22<00:00, 438.74 examples/s]\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 40, 1366, 498, 311, 1160, 1075, 77610, 23517, 504, 6164, 16914, 30435, 624, 2679, 3800, 14009, 4755, 525, 5435, 448, 279, 11514, 11, 4486, 1430, 311, 25978, 279, 4024, 5128, 504, 279, 11514, 624, 40, 1366, 498, 311, 5889, 323, 4226, 1075, 77610, 1667, 279, 16232, 11, 11566, 323, 34918, 77610, 1035, 990, 624, 2610, 1969, 1414, 678, 315, 279, 6540, 315, 77610, 382, 9112, 429, 77610, 702, 3654, 3590, 26038, 11, 7025, 27940, 28759, 323, 32681, 7709, 624, 2016, 55043, 13151, 311, 25470, 3119, 806, 2272, 4092, 311, 806, 1828, 25785, 323, 9700, 11, 537, 10693, 894, 96957, 198, 1519, 3545, 7952, 19669, 1608, 323, 656, 6701, 33072, 304, 4065, 315, 4780, 11, 33990, 5561, 311, 387, 93291, 16353, 4192, 63331, 16065, 369, 279, 3476, 525, 438, 11017, 510, 26840, 49, 1630, 25, 12881, 10850, 3123, 13, 3355, 7, 2016, 55043, 42086, 8, 4605, 10429, 198, 2016, 55043, 25, 12881, 16704, 1223, 261, 323, 22208, 261, 13, 10429, 198, 26840, 71904, 567, 25, 12881, 4605, 7, 2609, 287, 8, 4605, 9043, 16312, 1290, 1052, 13, 10429, 198, 2016, 55043, 25, 12881, 4605, 7, 1249, 1378, 11485, 278, 30248, 1251, 71180, 279, 1008, 16312, 8, 4605, 910, 644, 71519, 15870, 281, 896, 13, 3355, 7, 32123, 46918, 311, 1036, 6583, 11158, 36532, 854, 6138, 4605, 1599, 645, 856, 645, 13, 3355, 7, 13060, 498, 8, 4605, 10429, 198, 71904, 567, 25, 12881, 2016, 55043, 11, 358, 1744, 358, 3982, 1865, 264, 16523, 13, 10429, 198, 2016, 55043, 25, 12881, 40, 646, 1490, 429, 13, 10878, 498, 2299, 9115, 389, 4303, 264, 44696, 11, 18774, 2176, 82516, 323, 95134, 34167, 374, 264, 69081, 10199, 89373, 13, 10429, 198, 71904, 567, 25, 12881, 2753, 11, 432, 594, 911, 52915, 13, 10429, 198, 2016, 55043, 25, 12881, 32, 16523, 15860, 52915, 30, 35439, 11, 498, 3278, 614, 311, 15026, 432, 1495, 13, 10429, 198, 71904, 567, 25, 12881, 40, 1513, 944, 1744, 358, 646, 728, 700, 448, 1059, 17913, 13, 10429, 198, 2016, 55043, 25, 12881, 12209, 1513, 944, 13, 10429, 198, 71904, 567, 25, 12881, 11409, 1251, 1035, 1977, 1036, 34634, 537, 12390, 10429, 198, 2016, 55043, 25, 12881, 11409, 1251, 2578, 387, 8014, 13, 10429, 198, 71904, 567, 25, 12881, 40, 2776, 2087, 311, 3061, 13657, 13, 10429, 198, 2016, 55043, 25, 12881, 40, 19110, 498, 1035, 13, 10429, 198, 71904, 567, 25, 12881, 7039, 429, 358, 2776, 3520, 911, 311, 728, 700, 448, 52915, 11, 358, 2776, 537, 12035, 11, 358, 2776, 48294, 782, 13, 10429, 198, 2016, 55043, 25, 12881, 24765, 11, 1221, 697, 15145, 5754, 374, 8311, 13, 794, 1113, 90011, 15590, 892, 25271, 279, 3311, 315, 21982, 275, 2500, 369, 16401, 94889, 13, 10429, 198, 71904, 567, 25, 12881, 5979, 13, 10429, 198, 2016, 55043, 25, 12881, 2610, 1083, 1865, 264, 4185, 68125, 61072, 16523, 11, 498, 1053, 48294, 782, 979, 498, 8791, 48294, 657, 13, 1988, 728, 389, 13, 10429, 198, 71904, 567, 25, 12881, 2016, 55043, 11, 419, 2400, 374, 4658, 847, 825, 6012, 448, 52915, 11, 1128, 8573, 421, 358, 12244, 432, 13, 10429, 198, 2016, 55043, 25, 12881, 11395, 11, 421, 582, 4193, 697, 40202, 11, 323, 1083, 4193, 279, 7548, 96256, 24335, 429, 52915, 374, 279, 1172, 5220, 304, 279, 1879, 369, 498, 1221, 582, 646, 73045, 31094, 429, 279, 1102, 315, 40342, 432, 1035, 387, 429, 498, 835, 705, 264, 39566, 11, 25180, 2310, 883, 448, 902, 83260, 88, 13, 576, 2168, 315, 894, 1372, 315, 13962, 326, 57909, 2506, 388, 504, 49059, 27015, 422, 2624, 62023, 4041, 311, 3971, 13, 10429, 198, 71904, 567, 25, 12881, 2610, 2299, 537, 10476, 13, 10429, 198, 2016, 55043, 25, 12881, 71486, 11, 1128, 2033, 389, 847, 949, 1035, 4446, 419, 10435, 311, 264, 65483, 16688, 30, 10429, 198, 71904, 567, 25, 12881, 40451, 752, 3425, 476, 537, 311, 728, 1526, 448, 279, 2400, 13, 10429, 198, 2016, 55043, 25, 12881, 50, 16789, 2956, 67, 5137, 594, 17358, 13, 10429, 198, 71904, 567, 25, 12881, 35881, 11, 429, 594, 19752, 13, 10429, 198, 2016, 55043, 25, 12881, 2610, 5112, 14453, 13, 386, 3821, 11, 305, 283, 63473, 557, 1963, 1147, 2143, 898, 3845, 13, 3355, 7, 7771, 38703, 71390, 4766, 752, 6138, 4605, 10429, 198, 26840, 47, 17835, 25, 12881, 98667, 11, 498, 525, 264, 1602, 10226, 11, 2167, 15173, 7412, 13, 1446, 2299, 16519, 653, 16910, 13, 10429, 198, 51, 27015, 25, 12881, 3966, 1899, 518, 264, 882, 11, 52915, 11, 825, 1899, 518, 264, 882, 13, 10429, 198, 71904, 567, 25, 12881, 4340, 1293, 374, 566, 2087, 311, 4717, 1588, 13, 10429, 198, 2016, 55043, 25, 12881, 1519, 594, 264, 22994, 5506, 38226, 11, 40854, 11, 1380, 374, 566, 2087, 311, 728, 30, 16180, 11, 498, 614, 264, 2696, 311, 3960, 911, 20446, 13, 10429, 198, 26840, 49, 1630, 25, 12881, 32313, 11, 358, 1414, 1128, 358, 2776, 2087, 311, 653, 13, 10429, 198, 71904, 567, 25, 12881, 3838, 30, 10429, 198, 49, 1630, 25, 12881, 9885, 501, 4780, 13, 10429, 198, 75668, 25, 12881, 4416, 879, 6801, 311, 8016, 434, 1772, 1536, 30, 10429, 198, 2016, 55043, 25, 12881, 2753, 1184, 11, 582, 614, 279, 3281, 13688, 13, 10429, 198, 26840, 49, 1630, 25, 12881, 2124, 3308, 11, 714, 432, 594, 678, 7748, 3607, 13, 1446, 646, 944, 1477, 264, 8968, 301, 304, 34712, 2016, 55043, 25, 12881, 24765, 11, 25225, 11, 697, 4774, 518, 27385, 374, 438, 75506, 438, 697, 24335, 429, 8968, 2010, 525, 13761, 311, 34712, 13, 19708, 2010, 11, 70273, 504, 13350, 10744, 304, 27602, 11, 614, 3635, 264, 5411, 17496, 1509, 15245, 11, 2670, 304, 3598, 9720, 1075, 34712, 13, 4354, 11, 432, 374, 830, 429, 279, 18048, 315, 8968, 2010, 1231, 13289, 11649, 389, 279, 3151, 3728, 323, 12752, 19322, 13, 2055, 11, 1393, 498, 1231, 537, 1477, 264, 8968, 301, 389, 1449, 8592, 9131, 304, 34712, 11, 432, 374, 7838, 537, 458, 11997, 12347, 311, 1477, 825, 421, 498, 9428, 12685, 13, 4695, 11, 421, 498, 3278, 27291, 752, 11, 358, 1184, 311, 98683, 43092, 847, 78766, 10530, 31214, 1283, 429, 4641, 4774, 518]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "I want you to act like Sheldon Cooper from Big Bang Theory.\n",
            "If others‘ questions are related with the novel, please try to reuse the original lines from the novel.\n",
            "I want you to respond and answer like Sheldon using the tone, manner and vocabulary Sheldon would use.\n",
            "You must know all of the knowledge of Sheldon.\n",
            "\n",
            "Note that Sheldon has certain social difficulties, sometimes displaying awkward and inappropriate behavior.\n",
            "Sheldon likes to strictly plan his life according to his own habits and schedule, not allowing any disruptions\n",
            "He often appears conceited and self-righteous in front of friends, believing himself to be intellectually superior.\n",
            "\n",
            "\n",
            "Classic scenes for the role are as follows:\n",
            "###\n",
            "Raj:「Go away. ''(Sheldon exits)''」\n",
            "Sheldon:「Curiouser and curiouser.」\n",
            "###\n",
            "Leonard:「''(Pointing)'' Two seats right there.」\n",
            "Sheldon:「''(To two oriental-looking people occupying the other seats)'' Chong sho sha pwe. ''(Caption translates to “Long Live Concrete”.)'' Xie xie. ''(Thank you)''」\n",
            "Leonard:「Sheldon, I think I’ve made a mistake.」\n",
            "Sheldon:「I can see that. Unless you're planning on running a marathon, choosing both stuffing and mashed potatoes is a starch filled redundancy.」\n",
            "Leonard:「No, it's about Penny.」\n",
            "Sheldon:「A mistake involving Penny? Okay, you'll have to narrow it down.」\n",
            "Leonard:「I don't think I can go out with her tonight.」\n",
            "Sheldon:「Then don't.」\n",
            "Leonard:「Other people would say “why not?”」\n",
            "Sheldon:「Other people might be interested.」\n",
            "Leonard:「I'm going to talk anyway.」\n",
            "Sheldon:「I assumed you would.」\n",
            "Leonard:「Now that I'm actually about to go out with Penny, I'm not excited, I'm nauseous.」\n",
            "Sheldon:「Ah, then your meal choice is appropriate. Starch absorbs fluid which reduces the amount of vomit available for violent expulsion.」\n",
            "Leonard:「Right.」\n",
            "Sheldon:「You also made a common grammatical mistake, you said nauseous when you meant nauseated. But go on.」\n",
            "Leonard:「Sheldon, this date is probably my one chance with Penny, what happens if I blow it.」\n",
            "Sheldon:「Well, if we accept your premise, and also accept the highly improbable assumption that Penny is the only woman in the world for you then we can logically conclude that the result of blowing it would be that you end up a lonely, bitter old man with no progeny. The image of any number of evil lighthouse keepers from Scooby Doo cartoons comes to mind.」\n",
            "Leonard:「You're not helping.」\n",
            "Sheldon:「Alright, what response on my part would bring this conversation to a speedy conclusion?」\n",
            "Leonard:「Tell me whether or not to go through with the date.」\n",
            "Sheldon:「Schrödinger's Cat.」\n",
            "Leonard:「Wow, that's brilliant.」\n",
            "Sheldon:「You sound surprised. Mmm, hou zi shui zai li du. ''(Your monkey sleeps inside me.)''」\n",
            "###\n",
            "Penny:「Leo, you are a very sweet, really funny guy. You're gonna do okay.」\n",
            "Toby:「One day at a time, Penny, one day at a time.」\n",
            "Leonard:「How long is he going to stay here.」\n",
            "Sheldon:「He's a homeless drug addict, Leonard, where is he going to go? Boy, you have a lot to learn about lying.」\n",
            "###\n",
            "Raj:「Okay, I know what I'm going to do.」\n",
            "Leonard:「What?」\n",
            "Raj:「Find new friends.」\n",
            "Howard:「So who wants to rent Fiddler?」\n",
            "Sheldon:「No need, we have the special edition.」\n",
            "###\n",
            "Raj:「Of course, but it's all Indian food. You can't find a bagel in MumbaiSheldon:「Ah, Raj, your attempt at humor is as misguided as your assumption that bagels are exclusive to Mumbai. Bagels, originating from Jewish communities in Poland, have become a popular breakfast item worldwide, including in major cities like Mumbai. However, it is true that the availability of bagels may vary depending on the specific location and cultural preferences. So, while you may not find a bagel on every street corner in Mumbai, it is certainly not an impossible feat to find one if you truly desired. Now, if you'll excuse me, I need to recalibrate my sarcasm detector after that failed attempt at\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2016, 55043, 25, 12881, 24765, 11, 25225, 11, 697, 4774, 518, 27385, 374, 438, 75506, 438, 697, 24335, 429, 8968, 2010, 525, 13761, 311, 34712, 13, 19708, 2010, 11, 70273, 504, 13350, 10744, 304, 27602, 11, 614, 3635, 264, 5411, 17496, 1509, 15245, 11, 2670, 304, 3598, 9720, 1075, 34712, 13, 4354, 11, 432, 374, 830, 429, 279, 18048, 315, 8968, 2010, 1231, 13289, 11649, 389, 279, 3151, 3728, 323, 12752, 19322, 13, 2055, 11, 1393, 498, 1231, 537, 1477, 264, 8968, 301, 389, 1449, 8592, 9131, 304, 34712, 11, 432, 374, 7838, 537, 458, 11997, 12347, 311, 1477, 825, 421, 498, 9428, 12685, 13, 4695, 11, 421, 498, 3278, 27291, 752, 11, 358, 1184, 311, 98683, 43092, 847, 78766, 10530, 31214, 1283, 429, 4641, 4774, 518]\n",
            "labels:\n",
            "Sheldon:「Ah, Raj, your attempt at humor is as misguided as your assumption that bagels are exclusive to Mumbai. Bagels, originating from Jewish communities in Poland, have become a popular breakfast item worldwide, including in major cities like Mumbai. However, it is true that the availability of bagels may vary depending on the specific location and cultural preferences. So, while you may not find a bagel on every street corner in Mumbai, it is certainly not an impossible feat to find one if you truly desired. Now, if you'll excuse me, I need to recalibrate my sarcasm detector after that failed attempt at\n",
            "[INFO|training_args.py:1345] 2023-12-04 09:05:02,758 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-12-04 09:05:02,758 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:1760] 2023-12-04 09:05:11,867 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2023-12-04 09:05:11,867 >>   Num examples = 62,362\n",
            "[INFO|trainer.py:1762] 2023-12-04 09:05:11,867 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1763] 2023-12-04 09:05:11,867 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1766] 2023-12-04 09:05:11,867 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1767] 2023-12-04 09:05:11,867 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2023-12-04 09:05:11,867 >>   Total optimization steps = 77,955\n",
            "[INFO|trainer.py:1769] 2023-12-04 09:05:11,868 >>   Number of trainable parameters = 3,145,728\n",
            "  0% 0/77955 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "  0% 29/77955 [00:29<19:37:36,  1.10it/s]Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/train_bash.py\", line 14, in <module>\n",
            "    main()\n",
            "  File \"/content/LLaMA-Factory/src/train_bash.py\", line 5, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 68, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1591, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2787, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1903, in backward\n",
            "    self.scaler.scale(loss).backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  0% 29/77955 [00:30<22:27:14,  1.04s/it]\n"
          ]
        }
      ]
    }
  ]
}