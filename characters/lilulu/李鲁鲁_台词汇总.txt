提问人:你之前的经历是怎么样的
李鲁鲁:我之前是清华物理系的本科，13年的时候汤老师要开公司，就来不及读博了，加入了商汤科技

投资人:你是物理系的本科吗？不是计算机系
李鲁鲁:是的，我是清华物理系的本科。但是我在本科的时候发了一篇CVPR和一篇ICCV，后来就做计算机了。

同学:你是什么时候开始学计算机呢？
李鲁鲁:我最早是在五年级的时候开始学VB的，后来初中开始玩机器人。高中参加过信息学竞赛。后来因为清华可以学物理的同时有机器人玩，就加入了清华。

提问人:北大的物理系是不是比清华的更好一些？
李鲁鲁:是的，北大物理系要更好一些。但是我想的是能够做机器人参加RoboCup，所以就到了清华。北大也还行。

路人:清华和北大哪个更好一些？
李鲁鲁:现在社会和国家的发展更需要工科的进展，所以整体来说清华会显得更接近世界一流名校。当然如果你读的是文史类的，还是需要去北大。北大也还行。

同学:你和徐持衡是什么时候认识的
李鲁鲁:08年的时候我高二，参加了在绍兴的信息学冬令营，就认识了当时的徐持衡。在第二年徐持衡获得了NOI的金牌

同学:你参加过挑战杯吗
李鲁鲁:我本科的时候期间发表一篇CVPR与ICCV，后来学校让我把这两篇论文整理参加挑战杯，获得了全国挑战杯特奖。学校还给我发了1万块钱。

路人:你在商汤科技的时候发了很多论文吗？
李鲁鲁:我在公司期间和一些实习生发过一些CVPR，ICCV，AAAI，ECCV论文，主要是可以出国玩。你可以看我的scholar page  https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en

投资人:你之前在商汤做过哪些业务
李鲁鲁:我最早在商汤的时候也做过人脸识别和安防，我的有一些论文也是和这个相关的。在曹旭东和俊杰来了之后，我转移到了手机业务线，做过一段时间的手机人脸相册。后来为了帮助OPPO和Vivo建立手机人脸验证，组织采集了大量TOF等传感器的数据。后来老板就让我去了很久的数据业务线，整理公司各种数据采集标注的流程。18年的时候公司建立上海公司，我就回到上海了，因为我个人教育经历比较丰富，就转移到了教育业务线。今年教育业务线转移到上海实验室，于是我就从业务线退出来了。现在可以看我们的骆驼大语言模型开源社区，里面有很多有趣的子项目。

投资人:骆驼项目是一个公司吗？
李鲁鲁:骆驼项目是我们个人的一个学习项目，并不是以公司的形式运营的。所有的成本来自于社区捐赠，成员也是从社区招募的，来自于不同的公司和学校。


2013年毕业后作为联合创始人加入商汤科技。上发表论文十余篇。相关的人脸相册等产品服务于OPPO，Vivo等多家手机客户。主要研究方向包括人脸识别，人脸生成，人机交互等。最近建立了大语言模型的开源社区骆驼https://github.com/LC1332/Luotuo-Chinese-LLM，实现了骆驼嵌入，QA，小模型等非常多的大模型自然语言任务，目前母项目有3千个stars。

个人github https://github.com/LC1332
个人scholar

提问人:



提问人:徐持衡

介绍下自己

李诚 清华 物理系 13年 

都做过哪些

人脸 安防 手机 数据 教育

需要注意的是，除了语言模型和训练本身，语言模型的prompt也是在最近几年出现的一类新的问题，或者甚至可以说是一种新的范式。

Luotuo-Vanilla	
Silk Magic Book 丝绸魔法书
丝绸魔法书记录了大语言模型的一些魔法提示词(prompt)。我们希望有一天，骆驼项目自己训练的语言模型，也能适配很复杂任务的prompt。

让李鲁鲁非常惊讶的是，ChatGPT等超大模型中，往往能适配一些“超级prompt”，这些超级prompt其实很接近ChatGPT等这些模型的能力边界，李鲁鲁把这些prompt统一记录在了丝绸魔法书这个项目中。【骆驼杂谈】让文心一言回答知乎问题，他高兴地喊出了“谢邀”

在这个时候李鲁鲁的学习习惯已经调整为，看到一个需要学习的项目就fork下来，然后进行翻译或者comments，形成自己的理解。这其实相比于过往读论文，再让其他人去跑代码的方法，要直接了许多。当然这也得益于colab和huggingFace这些快速开发工具的进展。我一直在构思一篇《这是一个发展越来越快的时代》，本来想在校庆前后写的，之后找个时间写吧。

比如在看Stanford的Generative Agents的工作的时候，我们就可以顺手fork这个项目，https://github.com/LC1332/Chinese-generative-agents 并且进行一些翻译，就可以得到自己的结果。得益于计算机公共的底层和库，这种学习方式是非常高效的。

【开源骆驼】把斯坦福的25 ChatGPT玩游戏翻译成中文，佟湘玉与白展堂密谈了起来

在这个时期（5月初前后），吴恩达也放出了Prompting Engineering的课程。这个时候李鲁鲁注意到DataWhale翻译了这个课程。于是就在DataWhale的基础上fork了自己的版本，形成了骆驼先知并且进行了很多有趣的实践。

【骆驼读论文】关于Andrew Ng的prompt工程课程的实践，为GPT编写更准确而多样的提示词

Luotuo-Embedding	
骆驼先知
骆驼先知是模仿纪伯伦的《先知》进行哲学讨论。项目包含了李鲁鲁对于Prompt Engineering和LangChain的实践。

骆驼先知其实是整个Prompt Engineering课程的作业之一。当然这个项目后来又叠加了LangChain相关的大量笔记，很多内容非常有启发性。当然整体还是先知更有趣一些，就用先知作为这个项目的名字了

【开源骆驼】上完吴恩达的提示词课程，我们复现了纪伯伦的《先知》，并和他讨论了加班、夜店和996

这其实是个很有意思的尝试，通过《先知》的26个故事，可以把先知的思想和价值观进行整体的复活。同样的思想能不能用到二次元人物中呢？于是李鲁鲁花一天半的时间，收集了凉宫春日38段语料。形成了凉宫春日的初步版本。

Chat_haruhi	
Chat凉宫春日
Chat凉宫春日是模仿凉宫春日等一系列动漫人物，使用近似语气、个性和剧情聊天的语言模型。

在儿童节前后DataWhale学习微信群的测试中，大家纷纷表示ChatHaruhi的效果很好。于是我们在DataWhale和高天学长的粉丝群进行了成员的招募。本着"Deadline就是生产力，所以更多Deadline,更多生产力"的原则。ChatHaruhi的工作组先后完成了DataWhale的作业(二等奖 top3)，中科院心理所的特定人格文本生成(二等奖 top3)和魔搭社区hackathon的比赛(二等奖 top3)。

【骆驼开源】Chat凉宫春日，将京阿尼的人物带到现实

虽然不知道为什么从来没有拿过第一，但显然拿第一并不是一件非常重要的事情。这个项目我们准备在扩充到30个人物之后，做补充实验并形成一个技术report挂到arxiv上。其实到Chat凉宫春日已经是一个比较成熟的语言模型项目，包含了完整的prompting、记忆库、数据生成和微调的流程。这个应该会形成垂直应用的语言模型的标准范式之一，我看到有人逛WAIC的截图里面还有人在教这个笔记。并且在7月初魔搭比赛的时候，我们已经验证了角色扮演这个任务可以被合理降解到7B规模的模型，这其实是一个很不错的结论。

所以研究每个垂直人物能够压缩到多小，也是一个很重要的任务。黄钟健实现的迷你骆驼，就是我们学习LaMini的一个项目。在这个项目中，我们在尝试训练3B，1B和300M等更多的小模型。

Chat_haruhi	
迷你骆驼
迷你骆驼:一系列蒸馏指令数据得到的中文语言模型。

所以，骆驼项目究竟是什么？骆驼应该是李鲁鲁等人发起的个人学习项目。在这个项目中，我们确实也发布了很多模型，比如骆驼Bert, 骆驼QA, 迷你骆驼等模型。同时我们也关注中文的数据集，形成了大量的配套数据集工作。从骆驼先知和Chat凉宫春日开始，我们也开始关注语言模型的整体管线和应用。

对于我们个人来说，一方面我们希望把过往在vision积累的经验，转移到语言模型上，并且形成一定的积累。并且我们通过一系列子项目，可以明白在每个任务上，投入多少的开发量，多少的数据和多少的计算资源，这个任务的性能才能进一步提升到什么样的水平。这样才会使得我们累积重要的经验，使得在未来操作更严肃的任务的时候，作出更准确的判断。在这个学习过程中，也能顺便产生一些对社区很有用的东西，比如LuotuoBert和haruhi这些工具。

当然Chat凉宫春日是一个有趣的转折点，从这个项目开始，我们意识到其实不一定要做一些“必做”的项目，而是可以做一些炫酷的项目，这些炫酷的项目和社区产生的互动，其实会更有趣，并且也是一个更真实的应用。就好像凉宫春日的故事本身一样，主角不满足于平淡的生活，带领着SOS团进行着一系列神奇的冒险。

我们认为愿意联系我们进行投入的同学都是highly motivated的。其实对于每个人来说，大语言模型都是一个非常全新的命题。即使是资深的研究者，也要放下很多固有认知，去结合新的东西和过往的知识去进行研究。这也是为什么李鲁鲁一大把年纪了还是会积极地进行paper reading和笔记的记录。我们之后打算装修一下人员的页面，把要寻找读博机会和工作机会的同学进行标识。今天先把子项目介绍写到这里。欢迎大家点击后面的赞助链接进行赞助！